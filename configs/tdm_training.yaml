defaults:
  - _self_
  - base_config
  - env: antmaze_umaze
experiment:
  name: "tdm_training"
env:
  total_frames: 100_000
  max_frames_per_traj: 200
  init_random_frames: 5000
  frames_per_batch: 1
  reset_at_each_iter: False
  collector_device: "cpu"
  storing_device: "cpu"
  device: "cpu"
  obs:
    height: 128
    width: 128
    dim: 3
    normalize: True
    standardization_stats_init_iter: 0
    standardize: False
  state:
    normalize: False
    standardize: False
  goal:
    normalize: False
    standardize: False
    latent_dim: 256
    reached_epsilon: 0.1
  keys_of_interest: ["observation", "state", "pixels_latent", "action", "goal_latent", "planning_horizon", "reward", "done"]
replay_buffer:
  max_size: 1_000_000
train:
  tdm_rollout_max_planning_horizon: null
  tdm_max_planning_horizon: 2
  tdm_planning_horizon_annealing: False
  tdm_planning_horizon_annealing_cycles: 1
  tdm_planning_horizon_annealing_ratio: 1.0
  tdm_terminate_when_goal_reached: False
  updates_per_step: 5
  reward_distance_type: "l1"
  num_trajs: 20
  batch_size: 1000
  reward_dim: 256
  target_update_freq: 2
  actor_learning_rate: 1e-4
  critic_learning_rate: 1e-3
  polyak_avg: 0.001
  noise_type: "OU"
  noise_mean: 0
  noise_std: 0.2
  noise_annealing_steps: 50_000
  noise_sigma_init: 1.0
  noise_sigma_end: 0.5
  target_policy_action_noise_clip: 0.1
  target_policy_action_noise_std: 0.1
  alg: 'tdm_td3'
  grad_norm_clipping: 10.0
models:
  device: "cpu"
  encoder_decoder:
    name: "vae_best_model_antmaze_umaze-v4"
    version: "1.0.0"
    download_path: "models/encoder_decoder/"
    encoder:
      hidden_dims: [16, 32, 64, 128]
      hidden_activation: "leaky_relu"
      leaky_relu_neg_slope: 0.2
      hidden_kernels: [4, 4, 4, 4]
      hidden_strides: [2, 2, 2, 2]
      hidden_paddings: [1, 1, 1, 1]
      use_batch_norm: True
    decoder:
      hidden_dims: [64, 32, 16]
      hidden_activation: "relu"
      hidden_kernels: [4, 4, 4]
      hidden_strides: [2, 2, 2]
      hidden_paddings: [1, 1, 1]
      output_kernel: 4
      output_stride: 2
      output_padding: 1
      use_batch_norm: True
  actor:
    model_type: "mlp"
    hidden_layers_out_features: [128, 128]
    hidden_activation_function_name: "relu"
    use_batch_norm: False
    output_activation_function_name: "tanh"
    in_keys: ["pixels_latent", "state", "goal_latent", "planning_horizon"]
  critic:
    model_type: "mlp"
    hidden_layers_out_features: [128, 128]
    hidden_activation_function_name: "relu"
    use_batch_norm: False
    output_activation_function_name: "identity"
    in_keys: ["pixels_latent", "state", "action", "goal_latent", "planning_horizon"]
    is_relative: False
logging:
  step_metrics: ["original_reward", "goal_latent_reached", "goal_latent_distance", "planning_horizon", "q_value"]
  episode_metrics: ["episode_return", "episode_return_original_reward", "episode_length"]
  reward_keys: ["reward", "original_reward"]
  step_freq: 200
  metrics_frames: 500
  video_log_step_freq: 400
  video_frames: 200
  image_log_step_freq: 1200
  image_log: True
  image_log_max_nb_trajs: 3
  image_log_traj_goal: True
  image_log_traj_first_frame: True
  image_log_traj_last_frame: True
