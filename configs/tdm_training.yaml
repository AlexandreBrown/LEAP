defaults:
  - _self_
  - base_config
  - env: antmaze_umaze
experiment:
  name: "tdm_training"
env:
  total_frames: 262144
  max_frames_per_traj: 64
  init_random_frames: 16384
  frames_per_batch: 1
  reset_at_each_iter: False
  collector_device: "cpu"
  storing_device: "cpu"
  device: "cpu"
  obs:
    height: 128
    width: 128
    dim: 3
    normalize: True
    standardization_stats_init_iter: 0
    standardize: False
  state:
    normalize: True
    standardize: False
  goal:
    normalize: True
    standardize: False
    latent_dim: 256
    reached_epsilon: 0.1
  keys_of_interest: ["observation", "pixels_latent", "state", "action", "goal_latent", "planning_horizon", "reward", "done", "desired_goal", "achieved_goal"]
replay_buffer:
  max_size: 1_048_576
train:
  tdm_rollout_max_planning_horizon: null
  tdm_max_planning_horizon: 20
  tdm_planning_horizon_annealing: True
  tdm_planning_horizon_annealing_cycles: 1
  tdm_planning_horizon_annealing_ratio: 0.5
  tdm_terminate_when_goal_reached: True
  updates_per_step: 10
  reward_distance_type: "squared_diff"
  num_trajs: 2
  batch_size: 128
  reward_dim: 256
  target_update_freq: 2
  actor_learning_rate: 5e-4
  critic_learning_rate: 1e-4
  polyak_avg: 0.001
  noise_type: "OU"
  noise_mean: 0
  noise_std: 0.2
  noise_annealing_steps: 262144
  noise_sigma_init: 1.0
  noise_sigma_end: 0.2
  target_policy_action_noise_clip: 0.2
  target_policy_action_noise_std: 0.1
  alg: 'tdm_td3'
  grad_norm_clipping: 10.0
models:
  device: "cpu"
  encoder_decoder:
    name: "vae_best_model_antmaze_umaze-v4"
    version: "1.0.0"
    download_path: "models/encoder_decoder/"
    encoder:
      hidden_dims: [16, 32, 64, 128]
      hidden_activation: "leaky_relu"
      leaky_relu_neg_slope: 0.2
      hidden_kernels: [4, 4, 4, 4]
      hidden_strides: [2, 2, 2, 2]
      hidden_paddings: [1, 1, 1, 1]
      use_batch_norm: True
    decoder:
      hidden_dims: [64, 32, 16]
      hidden_activation: "relu"
      hidden_kernels: [4, 4, 4]
      hidden_strides: [2, 2, 2]
      hidden_paddings: [1, 1, 1]
      output_kernel: 4
      output_stride: 2
      output_padding: 1
      use_batch_norm: True
  actor:
    model_type: "mlp"
    hidden_layers_out_features: [256, 256, 256]
    hidden_activation_function_name: "relu"
    use_batch_norm: False
    output_activation_function_name: "tanh"
    in_keys: ["pixels_latent", "state", "goal_latent", "planning_horizon"]
  critic:
    model_type: "mlp"
    hidden_layers_out_features: [256, 256, 256]
    hidden_activation_function_name: "relu"
    use_batch_norm: False
    output_activation_function_name: "identity"
    in_keys: ["pixels_latent", "state", "action", "goal_latent", "planning_horizon"]
    is_relative: False
logging:
  step_metrics: ["original_reward", "goal_latent_reached", "goal_latent_distance", "planning_horizon", "q_value", "goal_reached", "goal_distance"]
  episode_metrics: ["episode_return"]
  step_freq: 1024
  metrics_frames: 512
  video_log_step_freq: 2048
  video_frames: 256
